
params {

    result_dir = 'results/dia_batch_correction'
    report_dir = 'reports/dia_batch_correction'

    skyline.group_by_gene = false

    precursor_report_template = 'https://raw.githubusercontent.com/uw-maccosslab/nf-dia-batch-correction/master/resources/precursor_quality.skyr'
    replicate_report_template = 'https://raw.githubusercontent.com/uw-maccosslab/nf-dia-batch-correction/master/resources/replicate_quality.skyr'

    panorama.domain = 'https://panoramaweb.org'
    panorama.skyline_folder = null
    panorama.reports_folder = null

    documents = null

    // Normalization parameters
    normalize_db {
        // List of replicates to exclude from normalization and batch correction
        exclude_replicates = null

        // List of Skyline documents to exclude from normalization and batch correction
        exclude_projects = null

        // Normalization method to use. Options are 'DirectLFQ' and 'median'
        method = 'DirectLFQ'
    }

    // Batch correction parameters
    bc {
        // batch correction method
        method = 'combat'

        // Metadata keys corresponding to batch levels 1 and 2
        // If null, the project name is used as the batch variable
        batch1 = null
        batch2 = null

        // Metadata key(s) used to color PCA plots.
        // Can be either string or list. If null, batch and acquisition number are used to color plots.
        color_vars = null

        // Metadata key(s) to use as covariates for batch correction
        // Can be either string or list. If null, no covariates are used.
        covariate_vars = null

        // Metadata key indicating replicates which are controls for CV plots
        // If null, all replicates are used in CV distribution plot.
        control_key = null

        // Metadata value(s) mapping to control_key indicating whether a replicate is a control.
        control_values = null

        // File extension for standalone plots. If null, no standalone plots are produced.
        plot_ext = null
    }

    // AWS Batch params
    aws {
        region = 'us-west-2'
        batch.cliPath = '/usr/local/aws-cli/v2/current/bin/aws'
        batch.logsGroup = '/batch/tei-nextflow-batch'
        batch.maxConnections = 20
        batch.connectionTimeout = 10000
        batch.uploadStorageClass = 'INTELLIGENT_TIERING'
        batch.storageEncryption = 'AES256'
        batch.retryMode = 'standard'
    }
}

plugins {
    id 'nf-amazon'
}

docker {
    enabled = true
}

// Manifest
manifest {
    name            = 'nf-dia-batch-correction'
    author          = 'Aaron Maurais'
    homePage        = 'https://github.com/uw-maccosslab/nf-dia-batch-correction'
    description     = 'Perform normalization and batch correction on one or more Skyline documents'
    mainScript      = 'main.nf'
    nextflowVersion = '!>=21.10.3'
}

aws {
    batch {
        // NOTE: this setting is only required if the AWS CLI tool is installed in a custom AMI
        cliPath = params.aws.batch.cliPath
        logsGroup = params.aws.batch.logsGroup
        maxConnections = params.aws.batch.maxConnections
        connectionTimeout = params.aws.batch.connectionTimeout
        uploadStorageClass = params.aws.batch.uploadStorageClass
        storageEncryption = params.aws.batch.storageEncryption
        retryMode = params.aws.batch.retryMode
    }

    region = params.aws.region
}

// Execution Profiles
profiles {

    /*
     * Params for running pipeline on the local computer (e.g.:
     * your laptop). These can be overridden in the local config file.
     */
    standard {
        process.executor = 'local'

        executor.queueSize = 8

        params.max_memory = '12.GB'
        params.max_cpus = 8
        params.max_time = '240.h'

        // where to cache mzml files after running msconvert
        params.mzml_cache_directory = null
        params.panorama_cache_directory = null
    }
    aws {
        process.executor = 'awsbatch'
        // process.queue = 'nextflow_basic_ec2_1tb'
        process.queue = 'nextflow_basic_ec2'

        // params for running pipeline on aws batch
        // These can be overridden in local config file

        // max params allowed for your AWS Batch compute environment
        params.max_memory = '124.GB'
        params.max_cpus = 32
        params.max_time = '240.h'

        // where to cache mzml files after running msconvert
        params.mzml_cache_directory = 's3://mc-tei-rex-nextflow-dda/dia/mzml_cache'
        params.panorama_cache_directory = 's3://mc-tei-rex-nextflow-dda/panorama_cache'
    }
}

// Capture exit codes from upstream processes when piping
process.shell = ['/bin/bash', '-euo', 'pipefail']
def trace_timestamp = new java.util.Date().format( 'yyyy-MM-dd_HH-mm-ss')
timeline {
    enabled = true
    file    = "${params.report_dir}/execution_timeline_${trace_timestamp}.html"
}
report {
    enabled = true
    file    = "${params.report_dir}/execution_report_${trace_timestamp}.html"
}
trace {
    enabled = true
    file    = "${params.report_dir}/execution_trace_${trace_timestamp}.txt"
}
dag {
    enabled = false
    file    = "${params.report_dir}/pipeline_dag_${trace_timestamp}.html"
}

// Load base.config by default for all pipelines
includeConfig 'conf/base.config'

// Load the images to use for all processes
includeConfig 'conf/container_images.config'

